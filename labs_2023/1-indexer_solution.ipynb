{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1: Building an inverted index\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program that collects all the words from a set of documents\n",
    "* Build an index from the words\n",
    "* Represent a document using the Tf.Idf values\n",
    "* Write a short report of 1 to 2 pages on the assignment\n",
    "* Read a description of an industrial system and answer a question on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the missing code and run all the cells, you will show your notebook to one of the assistants to register that you have passed.\n",
    "\n",
    "In case, we do not have enough assistants, you will possibly submit your notebook to an automatic marking system. More information on this later if we need it. Do not erase the content of the cells as we will possibly check your programs manually.\n",
    "The submission instructions are at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will build an indexer to index all the words in a corpus. Conceptually, an index consists of rows with one word per row and the list of files and positions, where this word occurs. Such a row is called a _posting list_. You will encode the position of a word by the number of characters from the start of the file.\n",
    "<pre>\n",
    "word1: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "word2: file_name pos1 pos2 pos3... file_name pos1 pos2 ...\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports. Add others as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import regex as re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "from decimal import Decimal, getcontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create an index for a corpus of Selma Lagerlöf's works: To gather the corpus, you can alternatively:\n",
    "1. Download the <a href=\"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\">Selma folder</a> and uncompress it. It contains novels by <a href=\"https://sv.wikipedia.org/wiki/Selma_Lagerl%C3%B6f\">Selma Lagerlöf</a>. The text of these novels was extracted from <a href=\"https://litteraturbanken.se/forfattare/LagerlofS/titlar\">Lagerlöf arkivet</a> at <a href=\"https://litteraturbanken.se/\">Litteraturbanken</a>.\n",
    "2. Or run this cell that will download the corpus and place it in your folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selma has been downloaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma/bannlyst.txt',\n",
       " 'Selma/gosta.txt',\n",
       " 'Selma/herrgard.txt',\n",
       " 'Selma/jerusalem.txt',\n",
       " 'Selma/kejsaren.txt',\n",
       " 'Selma/marbacka.txt',\n",
       " 'Selma/nils.txt',\n",
       " 'Selma/osynliga.txt',\n",
       " 'Selma/troll.txt']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname) \n",
    "    for fname in \n",
    "    [\n",
    "        \"bannlyst.txt\", \n",
    "        \"gosta.txt\", \n",
    "        \"herrgard.txt\", \n",
    "        \"jerusalem.txt\", \n",
    "        \"kejsaren.txt\", \n",
    "        \"marbacka.txt\", \n",
    "        \"nils.txt\", \n",
    "        \"osynliga.txt\", \n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "    \n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "        \n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "            \n",
    "        print(\"Done!\")\n",
    "        \n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "    \n",
    "SELMA_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the indexer (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a production context, your final program would take a corpus as input (here the Selma Lagerlöf's novels) and create an index of all the words with their positions. You should be able to run it this way:\n",
    "<pre>$ python indexer.py folder_name</pre>\n",
    "In this lab, you will write the index in a Jupyter Notebook. The conversion into a Python program is left as an optional exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming the Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make programming easier, you will split this exercise into five steps:\n",
    "1. Index one file;\n",
    "2. Read the content of a folder\n",
    "3. Create a master index for all the files\n",
    "4. Use tfidf to represent the documents (novels)\n",
    "5. Compare the documents of a collection\n",
    "\n",
    "You will use dictionaries to represent the postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Write a program that reads one document <tt>file_name.txt</tt> and outputs an index file:\n",
    "            <tt>file_name.idx</tt>:\n",
    "        </p>\n",
    "        <ol>\n",
    "            <li>The index file will contain all the unique words in the document,\n",
    "                where each word is associated with the list of its positions in the document.\n",
    "            </li>\n",
    "            <li>You will represent this index as a dictionary, where the keys will be the words, and\n",
    "                the values, the lists of positions\n",
    "            </li>\n",
    "            <li>As words, you will consider all the strings of letters that you will set in lower case.\n",
    "                You will not index the rest (i.e. numbers, punctuations, or symbols).\n",
    "            </li>\n",
    "            <li>To extract the words, use Unicode regular expressions. Do not use <tt>\\w+</tt>,\n",
    "                for instance, but the Unicode equivalent.\n",
    "            </li>\n",
    "            <li>The word positions will correspond to the number of characters from the beginning of the file.\n",
    "                (The word offset from the beginning)\n",
    "            </li>\n",
    "            <li>You will use the <tt>finditer()</tt> method to find the positions of the words.\n",
    "                This will return you match objects,\n",
    "                where you will get the matches and the positions with\n",
    "                the <tt>group()</tt> and <tt>start()</tt> methods.\n",
    "            </li>\n",
    "            <li>You will use the pickle package to write your dictionary in an file,\n",
    "                see <a href=\"https://wiki.python.org/moin/UsingPickle\">https://wiki.python.org/moin/UsingPickle</a>.\n",
    "            </li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an excerpt of the index of the `bannlyst.txt` text for the words <i>gjord</i>, <i>uppklarnande</i>, and <i>stjärnor</i>. The data is stored in a dictionary:\n",
    "\n",
    "<pre>\n",
    "{...\n",
    "'gjord': [8600, 183039, 220445],\n",
    "'uppklarnande': [8617],\n",
    "'stjärnor': [8641], ...\n",
    "}\n",
    "</pre>\n",
    "where the word <i>gjord</i> occurs three times in the text at positions 8600, 183039, and 220445, <i>uppklarnande</i>, once at position 8617, and <i>stjärnor</i>, once at position 8641."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Unicode regular expression to find words defined as sequences of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your regex here\n",
    "#regex = \"[A-Za-zåäöÅÄÖ]+\" \n",
    "regex = \"\\p{L}+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En',\n",
       " 'gång',\n",
       " 'hade',\n",
       " 'de',\n",
       " 'på',\n",
       " 'Mårbacka',\n",
       " 'en',\n",
       " 'barnpiga',\n",
       " 'som',\n",
       " 'hette',\n",
       " 'Back',\n",
       " 'Kajsa']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(regex, 'En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `regex`, write `tokenize(text)` function to tokenize a text. Return their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#def tokenize(text):\n",
    "#    #regex = \"[A-Öa-ö]+\"\n",
    "#    regex = \"[A-Za-zåäöÅÄÖ]+\" \n",
    "#    words = re.findall(regex, text)\n",
    "#\n",
    "#    res = []\n",
    "#    searchFromIndex = 0\n",
    "#    for word in words:\n",
    "#        match = re.search(word, text, pos = searchFromIndex)\n",
    "#        res.append(match)\n",
    "#        searchFromIndex = match.end()\n",
    "#    return res\n",
    "\n",
    "def tokenize(text):\n",
    "    return list(re.finditer(regex, text.lower().strip())) #Enl tre rutor ner så ska man göra .lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<regex.Match object; span=(0, 2), match='en'>,\n",
       " <regex.Match object; span=(3, 7), match='gång'>,\n",
       " <regex.Match object; span=(8, 12), match='hade'>,\n",
       " <regex.Match object; span=(13, 15), match='de'>,\n",
       " <regex.Match object; span=(16, 18), match='på'>,\n",
       " <regex.Match object; span=(19, 27), match='mårbacka'>,\n",
       " <regex.Match object; span=(28, 30), match='en'>,\n",
       " <regex.Match object; span=(31, 39), match='barnpiga'>,\n",
       " <regex.Match object; span=(41, 44), match='som'>,\n",
       " <regex.Match object; span=(45, 50), match='hette'>,\n",
       " <regex.Match object; span=(51, 55), match='back'>,\n",
       " <regex.Match object; span=(56, 61), match='kajsa'>]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.')\n",
    "list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `text_to_idx(words)` function to extract the indices from the list of tokens (words). Return a dictionary, where the keys will be the tokens (words), and the values a list of positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def text_to_idx(tokens):\n",
    "    res = dict()\n",
    "    for match_object in tokens:\n",
    "        word = match_object.group(0)\n",
    "        newValue = match_object.start()\n",
    "\n",
    "        if word in res:\n",
    "            oldValues = res.get(word)\n",
    "            values = oldValues + [newValue]\n",
    "            res.update({word: values})\n",
    "        else:\n",
    "            res.update({word: [newValue]})\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': [0, 28],\n",
       " 'gång': [3],\n",
       " 'hade': [8],\n",
       " 'de': [13],\n",
       " 'på': [16],\n",
       " 'mårbacka': [19],\n",
       " 'barnpiga': [31],\n",
       " 'som': [41],\n",
       " 'hette': [45],\n",
       " 'back': [51],\n",
       " 'kajsa': [56]}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize('En gång hade de på Mårbacka en barnpiga, som hette Back-Kajsa.'.lower().strip())\n",
    "text_to_idx(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one file, _Mårbacka_, `marbacka.txt`, set it in lowercase, tokenize it, and index it. Call this index `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def file_to_string(file_name):\n",
    "    return open(\"Selma/\" + file_name, \"r\").read().lower()#.replace('\\n', ' ')\n",
    "\n",
    "marb = file_to_string(\"marbacka.txt\")\n",
    "tokens = tokenize(marb)\n",
    "idx = text_to_idx(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 139,\n",
       " 752,\n",
       " 1700,\n",
       " 2582,\n",
       " 3324,\n",
       " 15117,\n",
       " 15404,\n",
       " 27794,\n",
       " 42175,\n",
       " 49126,\n",
       " 50407,\n",
       " 52053,\n",
       " 60144,\n",
       " 63374,\n",
       " 64910,\n",
       " 67182,\n",
       " 67330,\n",
       " 67799,\n",
       " 67824,\n",
       " 69232,\n",
       " 71328,\n",
       " 72099,\n",
       " 74147,\n",
       " 74255,\n",
       " 74614,\n",
       " 76610,\n",
       " 76884,\n",
       " 77138,\n",
       " 77509,\n",
       " 77787,\n",
       " 77936,\n",
       " 78574,\n",
       " 80597,\n",
       " 81782,\n",
       " 82003,\n",
       " 84363,\n",
       " 84786,\n",
       " 85251,\n",
       " 89837,\n",
       " 97093,\n",
       " 98642,\n",
       " 100474,\n",
       " 105063,\n",
       " 105298,\n",
       " 105721,\n",
       " 108710,\n",
       " 109133,\n",
       " 112844,\n",
       " 113725,\n",
       " 114997,\n",
       " 115583,\n",
       " 115833,\n",
       " 116368,\n",
       " 116557,\n",
       " 121896,\n",
       " 124823,\n",
       " 126409,\n",
       " 126542,\n",
       " 128758,\n",
       " 130976,\n",
       " 131939,\n",
       " 132826,\n",
       " 136914,\n",
       " 137187,\n",
       " 137872,\n",
       " 139196,\n",
       " 140721,\n",
       " 142324,\n",
       " 146781,\n",
       " 151497,\n",
       " 154335,\n",
       " 155139,\n",
       " 155438,\n",
       " 155886,\n",
       " 156405,\n",
       " 158108,\n",
       " 159817,\n",
       " 160107,\n",
       " 161158,\n",
       " 162085,\n",
       " 165847,\n",
       " 168316,\n",
       " 168528,\n",
       " 169111,\n",
       " 170333,\n",
       " 172684,\n",
       " 182047,\n",
       " 182427,\n",
       " 186362,\n",
       " 189535,\n",
       " 190999,\n",
       " 191110,\n",
       " 193177,\n",
       " 196686,\n",
       " 202552,\n",
       " 206340,\n",
       " 207789,\n",
       " 208382,\n",
       " 209874,\n",
       " 210525,\n",
       " 217464,\n",
       " 219933,\n",
       " 221393,\n",
       " 221533,\n",
       " 221880,\n",
       " 222213,\n",
       " 224190,\n",
       " 229501,\n",
       " 229598,\n",
       " 230783,\n",
       " 231453,\n",
       " 232140,\n",
       " 234427,\n",
       " 236193,\n",
       " 236950,\n",
       " 240168,\n",
       " 241891,\n",
       " 242359,\n",
       " 242934,\n",
       " 243030,\n",
       " 244831,\n",
       " 249882,\n",
       " 251277,\n",
       " 251901,\n",
       " 256360,\n",
       " 260244,\n",
       " 261612,\n",
       " 262384,\n",
       " 263856,\n",
       " 266638,\n",
       " 269760,\n",
       " 270113,\n",
       " 270674,\n",
       " 271146,\n",
       " 271885,\n",
       " 272560,\n",
       " 273464,\n",
       " 275086,\n",
       " 275664,\n",
       " 276207,\n",
       " 277407,\n",
       " 292648,\n",
       " 299762,\n",
       " 306277,\n",
       " 307507,\n",
       " 307972,\n",
       " 308148,\n",
       " 308330,\n",
       " 308568,\n",
       " 311856,\n",
       " 317491,\n",
       " 321194,\n",
       " 321925,\n",
       " 328154,\n",
       " 328470,\n",
       " 328977,\n",
       " 330435,\n",
       " 331650,\n",
       " 337494,\n",
       " 340526,\n",
       " 348636,\n",
       " 349331,\n",
       " 350022,\n",
       " 350168,\n",
       " 350674,\n",
       " 350949,\n",
       " 351349,\n",
       " 354175,\n",
       " 354411,\n",
       " 356314,\n",
       " 356541,\n",
       " 356788,\n",
       " 357522,\n",
       " 358413,\n",
       " 359478,\n",
       " 360511,\n",
       " 362108,\n",
       " 363675,\n",
       " 364467,\n",
       " 365049,\n",
       " 366954,\n",
       " 367286,\n",
       " 367741,\n",
       " 368878,\n",
       " 369487,\n",
       " 373757,\n",
       " 377123,\n",
       " 378852,\n",
       " 379696]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your index in a file so that you can reuse it. Use the pickle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "idx_file = open('idx_file.pickle', 'wb') # write bytes\n",
    "pickle.dump(idx, idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read back your file and store the content in `idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "idx_file = open('idx_file.pickle', 'rb') # read bytes\n",
    "idx = pickle.load(idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 139,\n",
       " 752,\n",
       " 1700,\n",
       " 2582,\n",
       " 3324,\n",
       " 15117,\n",
       " 15404,\n",
       " 27794,\n",
       " 42175,\n",
       " 49126,\n",
       " 50407,\n",
       " 52053,\n",
       " 60144,\n",
       " 63374,\n",
       " 64910,\n",
       " 67182,\n",
       " 67330,\n",
       " 67799,\n",
       " 67824,\n",
       " 69232,\n",
       " 71328,\n",
       " 72099,\n",
       " 74147,\n",
       " 74255,\n",
       " 74614,\n",
       " 76610,\n",
       " 76884,\n",
       " 77138,\n",
       " 77509,\n",
       " 77787,\n",
       " 77936,\n",
       " 78574,\n",
       " 80597,\n",
       " 81782,\n",
       " 82003,\n",
       " 84363,\n",
       " 84786,\n",
       " 85251,\n",
       " 89837,\n",
       " 97093,\n",
       " 98642,\n",
       " 100474,\n",
       " 105063,\n",
       " 105298,\n",
       " 105721,\n",
       " 108710,\n",
       " 109133,\n",
       " 112844,\n",
       " 113725,\n",
       " 114997,\n",
       " 115583,\n",
       " 115833,\n",
       " 116368,\n",
       " 116557,\n",
       " 121896,\n",
       " 124823,\n",
       " 126409,\n",
       " 126542,\n",
       " 128758,\n",
       " 130976,\n",
       " 131939,\n",
       " 132826,\n",
       " 136914,\n",
       " 137187,\n",
       " 137872,\n",
       " 139196,\n",
       " 140721,\n",
       " 142324,\n",
       " 146781,\n",
       " 151497,\n",
       " 154335,\n",
       " 155139,\n",
       " 155438,\n",
       " 155886,\n",
       " 156405,\n",
       " 158108,\n",
       " 159817,\n",
       " 160107,\n",
       " 161158,\n",
       " 162085,\n",
       " 165847,\n",
       " 168316,\n",
       " 168528,\n",
       " 169111,\n",
       " 170333,\n",
       " 172684,\n",
       " 182047,\n",
       " 182427,\n",
       " 186362,\n",
       " 189535,\n",
       " 190999,\n",
       " 191110,\n",
       " 193177,\n",
       " 196686,\n",
       " 202552,\n",
       " 206340,\n",
       " 207789,\n",
       " 208382,\n",
       " 209874,\n",
       " 210525,\n",
       " 217464,\n",
       " 219933,\n",
       " 221393,\n",
       " 221533,\n",
       " 221880,\n",
       " 222213,\n",
       " 224190,\n",
       " 229501,\n",
       " 229598,\n",
       " 230783,\n",
       " 231453,\n",
       " 232140,\n",
       " 234427,\n",
       " 236193,\n",
       " 236950,\n",
       " 240168,\n",
       " 241891,\n",
       " 242359,\n",
       " 242934,\n",
       " 243030,\n",
       " 244831,\n",
       " 249882,\n",
       " 251277,\n",
       " 251901,\n",
       " 256360,\n",
       " 260244,\n",
       " 261612,\n",
       " 262384,\n",
       " 263856,\n",
       " 266638,\n",
       " 269760,\n",
       " 270113,\n",
       " 270674,\n",
       " 271146,\n",
       " 271885,\n",
       " 272560,\n",
       " 273464,\n",
       " 275086,\n",
       " 275664,\n",
       " 276207,\n",
       " 277407,\n",
       " 292648,\n",
       " 299762,\n",
       " 306277,\n",
       " 307507,\n",
       " 307972,\n",
       " 308148,\n",
       " 308330,\n",
       " 308568,\n",
       " 311856,\n",
       " 317491,\n",
       " 321194,\n",
       " 321925,\n",
       " 328154,\n",
       " 328470,\n",
       " 328977,\n",
       " 330435,\n",
       " 331650,\n",
       " 337494,\n",
       " 340526,\n",
       " 348636,\n",
       " 349331,\n",
       " 350022,\n",
       " 350168,\n",
       " 350674,\n",
       " 350949,\n",
       " 351349,\n",
       " 354175,\n",
       " 354411,\n",
       " 356314,\n",
       " 356541,\n",
       " 356788,\n",
       " 357522,\n",
       " 358413,\n",
       " 359478,\n",
       " 360511,\n",
       " 362108,\n",
       " 363675,\n",
       " 364467,\n",
       " 365049,\n",
       " 366954,\n",
       " 367286,\n",
       " 367741,\n",
       " 368878,\n",
       " 369487,\n",
       " 373757,\n",
       " 377123,\n",
       " 378852,\n",
       " 379696]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['mårbacka']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the content of a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `get_files(dir, suffix)` function that reads all the files in a folder with a specific `suffix` (txt). You will need the Python `os` package, see <a href=\"https://docs.python.org/3/library/os.html\">https://docs.python.org/3/library/os.html</a>. You will return the file names in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reuse this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['troll.txt',\n",
       " 'kejsaren.txt',\n",
       " 'marbacka.txt',\n",
       " 'herrgard.txt',\n",
       " 'nils.txt',\n",
       " 'osynliga.txt',\n",
       " 'jerusalem.txt',\n",
       " 'bannlyst.txt',\n",
       " 'gosta.txt']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "get_file_names(\"Selma\", \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a master index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete your program with the creation of master index, where you will associate each word of the corpus with the files, where it occur and its positions: a posting list\n",
    "Below is an except of the master index with the words <i>samlar</i> and <i>ände</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samlar': {'troll.txt': [641880, 654233],\n",
       "  'nils.txt': [51805, 118943],\n",
       "  'osynliga.txt': [399121],\n",
       "  'gosta.txt': [313784, 409998, 538165]},\n",
       " 'ände': {'troll.txt': [39562, 650112],\n",
       "  'kejsaren.txt': [50171],\n",
       "  'marbacka.txt': [370324],\n",
       "  'nils.txt': [1794],\n",
       "  'osynliga.txt': [272144]}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'samlar':\n",
    "            {'troll.txt': [641880, 654233],\n",
    "            'nils.txt': [51805, 118943],\n",
    "            'osynliga.txt': [399121],\n",
    "            'gosta.txt': [313784, 409998, 538165]},\n",
    " 'ände':\n",
    "            {'troll.txt': [39562, 650112],\n",
    "            'kejsaren.txt': [50171],\n",
    "            'marbacka.txt': [370324],\n",
    "            'nils.txt': [1794],\n",
    "            'osynliga.txt': [272144]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word <i>samlar</i>, for instance, occurs three times in the gosta text at positions\n",
    "            313784, 409998, and 538165."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "#master_index = \n",
    "def create_master_index():\n",
    "    files = get_file_names(\"Selma\", \".txt\")\n",
    "\n",
    "    res = dict()\n",
    "    \n",
    "    for file in files:\n",
    "        file_text = file_to_string(file)\n",
    "        tokens = tokenize(file_text.lower())\n",
    "\n",
    "        words = text_to_idx(tokens)\n",
    "\n",
    "        for word in words:\n",
    "            occurance_in_file = {file: words[word]}\n",
    "            if word in res:\n",
    "                temp_dict = res.get(word)\n",
    "                temp_dict.update(occurance_in_file)\n",
    "                res.update({word: temp_dict})\n",
    "            else:\n",
    "                res.update({word: occurance_in_file})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'r'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m master_index \u001b[39m=\u001b[39m create_master_index()\n\u001b[1;32m      3\u001b[0m \u001b[39m#print(asd[:6])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#master_index['mårbacka']\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m master_index[\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'r'"
     ]
    }
   ],
   "source": [
    "#master_index['samlar']\n",
    "master_index = create_master_index()\n",
    "#print(asd[:6])\n",
    "master_index['mårbacka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [226291, 387634, 392959],\n",
       " 'marbacka.txt': [16,\n",
       "  139,\n",
       "  752,\n",
       "  1700,\n",
       "  2582,\n",
       "  3324,\n",
       "  15117,\n",
       "  15404,\n",
       "  27794,\n",
       "  42175,\n",
       "  49126,\n",
       "  50407,\n",
       "  52053,\n",
       "  60144,\n",
       "  63374,\n",
       "  64910,\n",
       "  67182,\n",
       "  67330,\n",
       "  67799,\n",
       "  67824,\n",
       "  69232,\n",
       "  71328,\n",
       "  72099,\n",
       "  74147,\n",
       "  74255,\n",
       "  74614,\n",
       "  76610,\n",
       "  76884,\n",
       "  77138,\n",
       "  77509,\n",
       "  77787,\n",
       "  77936,\n",
       "  78574,\n",
       "  80597,\n",
       "  81782,\n",
       "  82003,\n",
       "  84363,\n",
       "  84786,\n",
       "  85251,\n",
       "  89837,\n",
       "  97093,\n",
       "  98642,\n",
       "  100474,\n",
       "  105063,\n",
       "  105298,\n",
       "  105721,\n",
       "  108710,\n",
       "  109133,\n",
       "  112844,\n",
       "  113725,\n",
       "  114997,\n",
       "  115583,\n",
       "  115833,\n",
       "  116368,\n",
       "  116557,\n",
       "  121896,\n",
       "  124823,\n",
       "  126409,\n",
       "  126542,\n",
       "  128758,\n",
       "  130976,\n",
       "  131939,\n",
       "  132826,\n",
       "  136914,\n",
       "  137187,\n",
       "  137872,\n",
       "  139196,\n",
       "  140721,\n",
       "  142324,\n",
       "  146781,\n",
       "  151497,\n",
       "  154335,\n",
       "  155139,\n",
       "  155438,\n",
       "  155886,\n",
       "  156405,\n",
       "  158108,\n",
       "  159817,\n",
       "  160107,\n",
       "  161158,\n",
       "  162085,\n",
       "  165847,\n",
       "  168316,\n",
       "  168528,\n",
       "  169111,\n",
       "  170333,\n",
       "  172684,\n",
       "  182047,\n",
       "  182427,\n",
       "  186362,\n",
       "  189535,\n",
       "  190999,\n",
       "  191110,\n",
       "  193177,\n",
       "  196686,\n",
       "  202552,\n",
       "  206340,\n",
       "  207789,\n",
       "  208382,\n",
       "  209874,\n",
       "  210525,\n",
       "  217464,\n",
       "  219933,\n",
       "  221393,\n",
       "  221533,\n",
       "  221880,\n",
       "  222213,\n",
       "  224190,\n",
       "  229501,\n",
       "  229598,\n",
       "  230783,\n",
       "  231453,\n",
       "  232140,\n",
       "  234427,\n",
       "  236193,\n",
       "  236950,\n",
       "  240168,\n",
       "  241891,\n",
       "  242359,\n",
       "  242934,\n",
       "  243030,\n",
       "  244831,\n",
       "  249882,\n",
       "  251277,\n",
       "  251901,\n",
       "  256360,\n",
       "  260244,\n",
       "  261612,\n",
       "  262384,\n",
       "  263856,\n",
       "  266638,\n",
       "  269760,\n",
       "  270113,\n",
       "  270674,\n",
       "  271146,\n",
       "  271885,\n",
       "  272560,\n",
       "  273464,\n",
       "  275086,\n",
       "  275664,\n",
       "  276207,\n",
       "  277407,\n",
       "  292648,\n",
       "  299762,\n",
       "  306277,\n",
       "  307507,\n",
       "  307972,\n",
       "  308148,\n",
       "  308330,\n",
       "  308568,\n",
       "  311856,\n",
       "  317491,\n",
       "  321194,\n",
       "  321925,\n",
       "  328154,\n",
       "  328470,\n",
       "  328977,\n",
       "  330435,\n",
       "  331650,\n",
       "  337494,\n",
       "  340526,\n",
       "  348636,\n",
       "  349331,\n",
       "  350022,\n",
       "  350168,\n",
       "  350674,\n",
       "  350949,\n",
       "  351349,\n",
       "  354175,\n",
       "  354411,\n",
       "  356314,\n",
       "  356541,\n",
       "  356788,\n",
       "  357522,\n",
       "  358413,\n",
       "  359478,\n",
       "  360511,\n",
       "  362108,\n",
       "  363675,\n",
       "  364467,\n",
       "  365049,\n",
       "  366954,\n",
       "  367286,\n",
       "  367741,\n",
       "  368878,\n",
       "  369487,\n",
       "  373757,\n",
       "  377123,\n",
       "  378852,\n",
       "  379696],\n",
       " 'nils.txt': [991703]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#asd['samlar']\n",
    "master_index['mårbacka']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your master index in a file and read it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "#Write\n",
    "master_index_file = open('master_index.pickle', 'wb') # write bytes\n",
    "pickle.dump(master_index, master_index_file)\n",
    "\n",
    "#Read\n",
    "master_index_file = open('master_index.pickle', 'rb') # read bytes\n",
    "master_index = pickle.load(master_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [641880, 654233],\n",
       " 'nils.txt': [51805, 118943],\n",
       " 'osynliga.txt': [399121],\n",
       " 'gosta.txt': [313784, 409998, 538165]}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_index['samlar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `concordance(word, master_index, window)` function to extract the concordances of a `word` within a window of `window` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troll.txt\n",
      "\ten örtkunnig läkare, som samlar in markens växter \n",
      "\tälper dem, och medan hon samlar och handlar för de\n",
      "nils.txt\n",
      "\t bara, att du i all hast samlar ihop så mycket bos\n",
      "\tar stannat hemma, och nu samlar de sig för att int\n",
      "osynliga.txt\n",
      "\t till höger i kärran och samlar just ihop tömmarna\n",
      "gosta.txt\n",
      "\tom ligger nära borg, och samlar ihop ett litet mid\n",
      "\tlika förstämda.\n",
      "\n",
      "men hon samlar upp allt detta som\n",
      "\tn ensam i livet.\n",
      "\n",
      "därmed samlar han korten tillhop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "def concordance(word, master_index, window):\n",
    "    res = \"\"\n",
    "\n",
    "    mstr_idx = master_index[word]\n",
    "\n",
    "    for file_name in mstr_idx:\n",
    "        res += file_name\n",
    "        res += \"\\n\"\n",
    "\n",
    "        text = file_to_string(file_name)\n",
    "\n",
    "        \n",
    "        for pos in mstr_idx[file_name]:\n",
    "            steps_left = window\n",
    "            steps_right = window\n",
    "            if pos < steps_left:\n",
    "                diff = abs(pos - steps_left)\n",
    "                steps_right += diff\n",
    "                steps_left = pos\n",
    "\n",
    "            if len(text) < (pos + steps_right):\n",
    "                diff = abs(len(text) - (pos + steps_right))\n",
    "                steps_left += diff\n",
    "                steps_right -= diff\n",
    "\n",
    "            temp = text[pos - steps_left: pos + steps_right]\n",
    "            res += \"\t\" + temp + \"\\n\"\n",
    "\n",
    "\n",
    "    print(res)\n",
    "    #return res\n",
    "\n",
    "concordance(\"samlar\", master_index, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troll.txt\n",
      "\ten örtkunnig läkare, som samlar in markens växter \n",
      "\tälper dem, och medan hon samlar och handlar för de\n",
      "nils.txt\n",
      "\t bara, att du i all hast samlar ihop så mycket bos\n",
      "\tar stannat hemma, och nu samlar de sig för att int\n",
      "osynliga.txt\n",
      "\t till höger i kärran och samlar just ihop tömmarna\n",
      "gosta.txt\n",
      "\tom ligger nära borg, och samlar ihop ett litet mid\n",
      "\tlika förstämda.\n",
      "\n",
      "men hon samlar upp allt detta som\n",
      "\tn ensam i livet.\n",
      "\n",
      "därmed samlar han korten tillhop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concordance('samlar', master_index, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Documents with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the index, you will represent each document in your corpus as a dictionary. The keys of these dictionaries will be the words and you will define the value of a word with the tf-idf metric: \n",
    "1. Read the description of the tf-idf measure on Wikipedia (<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">https://en.wikipedia.org/wiki/Tf-idf</a>)\n",
    "2. After reading the description, you probably realized that there are multiple definitions of tf-idf. In this assignment, \n",
    " * Tf will be the relative frequency of the term in the document and \n",
    " * idf, the logarithm base 10 of the inverse document frequency.\n",
    "        \n",
    "You have below the tf-idf values for a few words. In our example, the word <i>gås</i> has the value 0 in bannlyst.txt and the value 0.000101001964 in nils.txt\n",
    "\n",
    "<pre>\n",
    "troll.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 2.148161748868631e-06\n",
    "\tet\t 0.0\n",
    "kejsaren.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 8.08284798629935e-06\n",
    "\tet\t 8.273225429362848e-05\n",
    "marbacka.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 7.582276564686669e-06\n",
    "\tet\t 9.70107989686256e-06\n",
    "herrgard.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "nils.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.00010100196417506702\n",
    "\tnils\t 0.00010164426900380124\n",
    "\tet\t 0.0\n",
    "osynliga.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "jerusalem.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 4.968292117670952e-06\n",
    "\tet\t 0.0\n",
    "bannlyst.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "gosta.txt\n",
    "\tkänna\t 0.0\n",
    "\tgås\t 0.0\n",
    "\tnils\t 0.0\n",
    "\tet\t 0.0\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the tf-idf representation is a vector. In your program, you will keep this idea and use all the words in the corpus as keys: Each dictionary will include all the words of the corpus as keys. The value of the key is then possibly 0, meaning that the word in not in the document or is in all the documents as for the word `nils` in `gosta.tx`. \n",
    "\n",
    "As further work, you may think of optimizing this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troll.txt\n",
      "tokens =  118867\n",
      "text.split =  119052\n",
      "text =  664346\n",
      "kejsaren.txt\n",
      "tokens =  63173\n",
      "text.split =  63125\n",
      "text =  338454\n",
      "marbacka.txt\n",
      "tokens =  67365\n",
      "text.split =  67256\n",
      "text =  380024\n",
      "herrgard.txt\n",
      "tokens =  33675\n",
      "text.split =  34244\n",
      "text =  185890\n",
      "nils.txt\n",
      "tokens =  198422\n",
      "text.split =  199267\n",
      "text =  1099212\n",
      "osynliga.txt\n",
      "tokens =  82749\n",
      "text.split =  83282\n",
      "text =  463414\n",
      "jerusalem.txt\n",
      "tokens =  154170\n",
      "text.split =  155365\n",
      "text =  859326\n",
      "bannlyst.txt\n",
      "tokens =  76083\n",
      "text.split =  76178\n",
      "text =  420340\n",
      "gosta.txt\n",
      "tokens =  129192\n",
      "text.split =  130246\n",
      "text =  733411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "def files_and_file_lengths():\n",
    "    files = get_file_names(\"Selma\", \".txt\")\n",
    "    file_lengths = dict() #spara här för att slippa för varenda ord\n",
    "\n",
    "    #Calculate file lengths\n",
    "    for file in files:\n",
    "        text = file_to_string(file)\n",
    "        tokens = tokenize(text)\n",
    "        file_lengths.update({file: len(tokens)})\n",
    "        #file_lengths.update({file: len(text.split())})\n",
    "        print(file)\n",
    "        print(\"tokens = \", len(tokens))\n",
    "        print(\"text.split = \", len(text.split()))\n",
    "        print(\"text = \", len(text))\n",
    "    return files, file_lengths\n",
    "\n",
    "    \n",
    "\n",
    "def tf_idf(word, file, file_lengths):\n",
    "    nbr_files = len(file_lengths)\n",
    "    word_occurances = master_index[word]\n",
    "    file_occurances = word_occurances.get(file, [])\n",
    "    nbr_occurances_in_this_file = len(file_occurances)\n",
    "    file_length = file_lengths[file]\n",
    "    \n",
    "    tf = nbr_occurances_in_this_file/file_length\n",
    "    #idf, the logarithm base 10 of the inverse document frequency.\n",
    "    if(len(word_occurances) == 0):\n",
    "        idf = 0\n",
    "    else:\n",
    "        #idf = math.log10(1 + nbr_files/(1 + len(word_occurances))) #Man plussar på ett för att inte dividera med 0\n",
    "        idf = math.log10(nbr_files/(len(word_occurances)))\n",
    "    \n",
    "    \n",
    "    return tf, idf\n",
    "\n",
    "def big_tf_idf_method():\n",
    "    files, file_lengths = files_and_file_lengths()\n",
    "    res = dict()\n",
    "\n",
    "    for word in master_index:\n",
    "        for file in files:\n",
    "            tf, idf = tf_idf(word, file, file_lengths)\n",
    "            old_values = res.get(file, {})\n",
    "            old_values.update({word: tf * idf})\n",
    "            res.update({file: old_values})\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "tfidf = big_tf_idf_method()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf['troll.txt']['känna'] #ska vara 0.0\n",
    "tfidf['troll.txt']['känna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.14754730163381e-06"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf['troll.txt']['nils'] #ska vara 2.148161748868631e-06\n",
    "#master_index[\"nils\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity, compare all the pairs of documents with their tf-idf representation and present your results in a table. You will include this table in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function computing the cosine similarity between two documents: `cosine_similarity(document1, document2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18105294879198452\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "def cosine_similarity(document1, document2):\n",
    "    #cosine similarity = (A, B) = (A ⋅ B) / (||A|| * ||B||)\n",
    "    v1 = tfidf[document1]\n",
    "    v2 = tfidf[document2]\n",
    "\n",
    "    union = v1.keys() & v2.keys()\n",
    "    #dot_product = sum(v1[word] * v2[word] for word in union) #gör i numpy istället?\n",
    "    v1_np = np.array([v1[word] for word in union])\n",
    "    v2_np = np.array([v2[word] for word in union])\n",
    "\n",
    "    #Kanske blir rätt om man inte använder numpy\n",
    "    #dot_product = sum(v1[word] * v2[word] for word in union) #gör i numpy istället?\n",
    "    #v1_magnitude = math.sqrt(sum(v1[word] ** 2 for word in union))\n",
    "    #v2_magnitude = math.sqrt(sum(v2[word] ** 2 for word in union))\n",
    "\n",
    "    ##dot_product = Decimal(np.dot(v1_np, v2_np)) #Något är fel men det är inte precisionen hmm\n",
    "    ##v1_magnitude = Decimal(np.linalg.norm(v1_np))\n",
    "    ##v2_magnitude = Decimal(np.linalg.norm(v2_np))\n",
    "\n",
    "    dot_product = np.dot(v1_np, v2_np) #Något är fel men det är inte precisionen hmm\n",
    "    v1_magnitude = np.linalg.norm(v1_np)\n",
    "    v2_magnitude = np.linalg.norm(v2_np)\n",
    "\n",
    "    similarity = dot_product / (v1_magnitude * v2_magnitude)\n",
    "\n",
    "    ##return float(similarity)\n",
    "    return similarity\n",
    "\n",
    "a = cosine_similarity(\"troll.txt\", \"kejsaren.txt\")\n",
    "print(a) #RÄTT!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity matrix between the documents of the corpus. While computing the similarities, you will record the two most similar documents that you will call `most_sim_doc1` and `most_sim_doc2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'troll.txt': {'troll.txt': 1.0, 'kejsaren.txt': 0.18105294879198452, 'marbacka.txt': 0.1471862973424799, 'herrgard.txt': 0.004068803287051186, 'nils.txt': 0.1883960991394727, 'osynliga.txt': 0.19258328779519387, 'jerusalem.txt': 0.007068692441175688, 'bannlyst.txt': 0.08858828535061439, 'gosta.txt': 0.19531265579473364}, 'kejsaren.txt': {'troll.txt': 0.18105294879198452, 'kejsaren.txt': 1.0000000000000002, 'marbacka.txt': 0.07113893882510008, 'herrgard.txt': 0.0007215810840719153, 'nils.txt': 0.04965067494010922, 'osynliga.txt': 0.051136753234210366, 'jerusalem.txt': 0.0018237239857823145, 'bannlyst.txt': 0.024018562861967352, 'gosta.txt': 0.04794431841966141}, 'marbacka.txt': {'troll.txt': 0.1471862973424799, 'kejsaren.txt': 0.07113893882510008, 'marbacka.txt': 0.9999999999999998, 'herrgard.txt': 0.003609829342629782, 'nils.txt': 0.08479100187538743, 'osynliga.txt': 0.09327694858730183, 'jerusalem.txt': 0.004875813209205003, 'bannlyst.txt': 0.03683400786525856, 'gosta.txt': 0.08009909257174144}, 'herrgard.txt': {'troll.txt': 0.004068803287051186, 'kejsaren.txt': 0.0007215810840719153, 'marbacka.txt': 0.003609829342629782, 'herrgard.txt': 1.0, 'nils.txt': 0.005063497319109268, 'osynliga.txt': 0.004825968661270903, 'jerusalem.txt': 0.3706735120524027, 'bannlyst.txt': 0.0009486946272284427, 'gosta.txt': 0.003104668610870954}, 'nils.txt': {'troll.txt': 0.1883960991394727, 'kejsaren.txt': 0.04965067494010922, 'marbacka.txt': 0.08479100187538743, 'herrgard.txt': 0.005063497319109268, 'nils.txt': 1.0, 'osynliga.txt': 0.11058855650383737, 'jerusalem.txt': 0.0045391616795896796, 'bannlyst.txt': 0.05098250777317043, 'gosta.txt': 0.10462088797101138}, 'osynliga.txt': {'troll.txt': 0.19258328779519387, 'kejsaren.txt': 0.051136753234210366, 'marbacka.txt': 0.09327694858730183, 'herrgard.txt': 0.004825968661270903, 'nils.txt': 0.11058855650383737, 'osynliga.txt': 0.9999999999999998, 'jerusalem.txt': 0.02831065870912324, 'bannlyst.txt': 0.05206571067785403, 'gosta.txt': 0.12450159309537476}, 'jerusalem.txt': {'troll.txt': 0.007068692441175688, 'kejsaren.txt': 0.0018237239857823145, 'marbacka.txt': 0.004875813209205003, 'herrgard.txt': 0.3706735120524027, 'nils.txt': 0.0045391616795896796, 'osynliga.txt': 0.02831065870912324, 'jerusalem.txt': 1.0, 'bannlyst.txt': 0.0064608125824917205, 'gosta.txt': 0.004311493950725538}, 'bannlyst.txt': {'troll.txt': 0.08858828535061439, 'kejsaren.txt': 0.024018562861967352, 'marbacka.txt': 0.03683400786525856, 'herrgard.txt': 0.0009486946272284427, 'nils.txt': 0.05098250777317043, 'osynliga.txt': 0.05206571067785403, 'jerusalem.txt': 0.0064608125824917205, 'bannlyst.txt': 1.0000000000000002, 'gosta.txt': 0.048941104096586176}, 'gosta.txt': {'troll.txt': 0.19531265579473364, 'kejsaren.txt': 0.04794431841966141, 'marbacka.txt': 0.08009909257174144, 'herrgard.txt': 0.003104668610870954, 'nils.txt': 0.10462088797101138, 'osynliga.txt': 0.12450159309537476, 'jerusalem.txt': 0.004311493950725538, 'bannlyst.txt': 0.048941104096586176, 'gosta.txt': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "def get_sim_matrix(files):\n",
    "    res = {}\n",
    "    for file_row in files:\n",
    "        if file_row not in res:\n",
    "            res[file_row] = {}\n",
    "        \n",
    "        for file_col in files:\n",
    "            sim = cosine_similarity(file_row, file_col) \n",
    "            res[file_row][file_col] = sim\n",
    "            #res[file_col][file_row] = sim\n",
    "    return res\n",
    "\n",
    "\n",
    "files = get_file_names(\"Selma\", \".txt\")\n",
    "similarity_matrix = get_sim_matrix(files)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the name of the two novels that are the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar: herrgard.txt jerusalem.txt Similarity: 0.3706735120524027\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(similarity_matrix):\n",
    "    max_so_far = 0\n",
    "    max_doc1 = \"\"\n",
    "    max_doc2 = \"\"\n",
    "    for file_row in files:\n",
    "        for file_col in files:\n",
    "            if file_row == file_col:\n",
    "                continue\n",
    "            if similarity_matrix[file_row][file_col] > max_so_far:\n",
    "                max_so_far = similarity_matrix[file_row][file_col]\n",
    "                max_doc1 = file_row\n",
    "                max_doc2 = file_col\n",
    "\n",
    "    return max_doc1, max_doc2, max_so_far\n",
    "\n",
    "\n",
    "\n",
    "#print(find_most_similar(similarity_matrix))\n",
    "most_sim_doc1, most_sim_doc2, max_similarity = find_most_similar(similarity_matrix)\n",
    "print(\"Most similar:\", most_sim_doc1, most_sim_doc2, \"Similarity:\", max_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, show it to one of the assistants who will pass you.\n",
    "\n",
    "In case, we do not have enough assitants, you will submit it to an automatic system. You will have more information later on this if we need it. In this case, fill in your IDs and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"ka4527ha-s\"] # Write your stil ids\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"1-indexer_solution.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the two most similar novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'herrgard.txt jerusalem.txt'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSWER = ' '.join(sorted([most_sim_doc1, most_sim_doc2]))\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karlhammar/Programmering/tempedan20/EDAN20/labs_2023/1-indexer_solution.ipynb.submission.bz2\n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\"\n",
    "print(CURRENT_NOTEBOOK_PATH + \".submission.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIGNMENT = 1\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='vilde.cs.lth.se', port=443): Max retries exceeded with url: /edan20checker/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1767fee10>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dns_host, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_conn()\n\u001b[1;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x1767fee10>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39me, _pool\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, _stacktrace\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='vilde.cs.lth.se', port=443): Max retries exceeded with url: /edan20checker/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1767fee10>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\u001b[39m\"\u001b[39m\u001b[39mhttps://vilde.cs.lth.se/edan20checker/submit\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                     files\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mnotebook_file\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mopen\u001b[39m(SUBMISSION_NOTEBOOK_PATH, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)}, \n\u001b[1;32m      3\u001b[0m                     data\u001b[39m=\u001b[39m{\n\u001b[1;32m      4\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mstil_id\u001b[39m\u001b[39m\"\u001b[39m: STIL_ID,\n\u001b[1;32m      5\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39massignment\u001b[39m\u001b[39m\"\u001b[39m: ASSIGNMENT,\n\u001b[1;32m      6\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: ANSWER,\n\u001b[1;32m      7\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mapi_key\u001b[39m\u001b[39m\"\u001b[39m: API_KEY,\n\u001b[1;32m      8\u001b[0m                     },\n\u001b[1;32m      9\u001b[0m                    verify\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display, JSON\n\u001b[1;32m     13\u001b[0m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url, data\u001b[39m=\u001b[39mdata, json\u001b[39m=\u001b[39mjson, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='vilde.cs.lth.se', port=443): Max retries exceeded with url: /edan20checker/submit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1767fee10>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "                   verify=False)\n",
    "\n",
    "\n",
    "from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `status` and be sure it is `correct`. If not, revise your code; verify that you obtained intermediate results identical to those in the notebook; and resubmit your notebook. You can submit multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Turning in your assigment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program, \n",
    "2. Read the text <i>Challenges in Building Large-Scale Information Retrieval Systems</i> about the history of <a href=\"https://research.google.com/people/jeff/WSDM09-keynote.pdf\">Google indexing</a> by <a href=\"https://research.google.com/pubs/jeff.html\">Jeff Dean</a> and write a short comment on it. See the details below.\n",
    "\n",
    "You will submit your report as well as your notebook (for archiving purposes) to Canvas: <https://canvas.education.lu.se/>. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (<a href=\"https://www.overleaf.com/\">www.overleaf.com</a>). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "In your report of about two pages: \n",
    "1. You will describe your indexer and comment your results; in this description, you will write the regular expression you used for the tokenization and include the similarity matrix;\n",
    "2. In Jeff Dean's document, you will identify the slide, where you have the most similar indexing technique. Please write the slide title and the slide number in your report.\n",
    "3. You will tell how your index encoding is related to what Google did.\n",
    "\n",
    "\n",
    "The submission deadline is September 15, 2023. You will have only three submission attempts. The deadline for the second and third ones are one week after you are noticed of your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
